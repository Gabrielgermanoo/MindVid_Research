{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (3.8.4)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.16-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 30.7/42.0 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 675.1 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.16-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.4/268.4 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Using cached joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.0 nltk-3.8.1 regex-2024.4.16 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gabriel_Germano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['é a maior é a melhor técnica e os estudos falam e a técnica mais conhecida no mundo que se chama Ground se consiste na ativação dos órgãos no sentido né muitos alunos fazem esse momentos difíceis as vésperas de provas e isso tem salvado tanta gente luds você pega o seu rosto agora nesse momento e aí você coloca cinco nos olhos quatro no nariz três na boca dois no ouvido e um lotado Então você com os olhos fechados você vai imaginar cinco coisas que você pode ver é para fechar cinco coisas Qualquer coisa Qualquer coisa quatro coisas que você pode sentir o cheiro três coisas que você pode sentir o sabor dois sons']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "df = pd.read_csv(\"./Videos/ansiedade_audio_transcriptions.csv\")\n",
    "df.head()\n",
    "sent_tokenize(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...  \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...  \n",
       "2  [ficar, olhando, para, as, anotações, para, as...  \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_words'] = df['Transcription'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gabriel_Germano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "      <td>[maior, melhor, técnica, estudos, falam, técni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "      <td>[fez, assim, assim, aí, fez, quê, príncipe, aí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "      <td>[ficar, olhando, anotações, mãos, deprimente, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "      <td>[4, coisas, precisa, saber, namora, pessoa, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...   \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...   \n",
       "2  [ficar, olhando, para, as, anotações, para, as...   \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...   \n",
       "\n",
       "                                   removed_stopwords  \n",
       "0  [maior, melhor, técnica, estudos, falam, técni...  \n",
       "1  [fez, assim, assim, aí, fez, quê, príncipe, aí...  \n",
       "2  [ficar, olhando, anotações, mãos, deprimente, ...  \n",
       "3  [4, coisas, precisa, saber, namora, pessoa, an...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['removed_stopwords'] = df['tokenized_words'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1-Score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel_Germano\\Documents\\Pesquisa\\MindVid_Research\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel_Germano\\Documents\\Pesquisa\\MindVid_Research\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Gabriel_Germano\\Documents\\Pesquisa\\MindVid_Research\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "y = df['tag']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Gabriel_Germano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "      <td>[maior, melhor, técnica, estudos, falam, técni...</td>\n",
       "      <td>[é, a, mai, é, a, melhor, técn, e, os, estud, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "      <td>[fez, assim, assim, aí, fez, quê, príncipe, aí...</td>\n",
       "      <td>[o, que, é, que, fez, assim, e, assim, e, voc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "      <td>[ficar, olhando, anotações, mãos, deprimente, ...</td>\n",
       "      <td>[fic, olh, par, as, anot, par, as, mão, é, dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "      <td>[4, coisas, precisa, saber, namora, pessoa, an...</td>\n",
       "      <td>[4, cois, que, voc, precis, sab, se, voc, nam,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...   \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...   \n",
       "2  [ficar, olhando, para, as, anotações, para, as...   \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  [maior, melhor, técnica, estudos, falam, técni...   \n",
       "1  [fez, assim, assim, aí, fez, quê, príncipe, aí...   \n",
       "2  [ficar, olhando, anotações, mãos, deprimente, ...   \n",
       "3  [4, coisas, precisa, saber, namora, pessoa, an...   \n",
       "\n",
       "                                            stemming  \n",
       "0  [é, a, mai, é, a, melhor, técn, e, os, estud, ...  \n",
       "1  [o, que, é, que, fez, assim, e, assim, e, voc,...  \n",
       "2  [fic, olh, par, as, anot, par, as, mão, é, dep...  \n",
       "3  [4, cois, que, voc, precis, sab, se, voc, nam,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "stemmer = RSLPStemmer()\n",
    "df['stemming'] = df['tokenized_words'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 107.3/107.3 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting jinja2 (from spacy)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.1.0-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Downloading spacy-3.7.4-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.7 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.7 MB 7.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.7 MB 8.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/11.7 MB 9.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/11.7 MB 9.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.8/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.2/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.0/11.7 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.7 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.7 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.7 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.7 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/11.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.9/11.7 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.7 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.7 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.7 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.7 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/11.7 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.0/182.0 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.4/122.4 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.3/409.3 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "   ---------------------------------------- 0.0/478.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 478.8/478.8 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.7/1.4 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 16.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.2/6.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.6/6.6 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.4 MB 17.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.4 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.8/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.1.0-cp312-cp312-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 150.9/150.9 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, marisa-trie, jinja2, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 jinja2-3.1.3 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmatizing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "      <td>[maior, melhor, técnica, estudos, falam, técni...</td>\n",
       "      <td>[é, a, mai, é, a, melhor, técn, e, os, estud, ...</td>\n",
       "      <td>[ser, o, grande, ser, o, bom, técnica, e, o, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "      <td>[fez, assim, assim, aí, fez, quê, príncipe, aí...</td>\n",
       "      <td>[o, que, é, que, fez, assim, e, assim, e, voc,...</td>\n",
       "      <td>[o, que, ser, que, fazer, assim, e, assim, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "      <td>[ficar, olhando, anotações, mãos, deprimente, ...</td>\n",
       "      <td>[fic, olh, par, as, anot, par, as, mão, é, dep...</td>\n",
       "      <td>[ficar, olhar, para, o, anotação, para, o, mão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "      <td>[4, coisas, precisa, saber, namora, pessoa, an...</td>\n",
       "      <td>[4, cois, que, voc, precis, sab, se, voc, nam,...</td>\n",
       "      <td>[4, coisa, que, você, precisar, saber, se, voc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...   \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...   \n",
       "2  [ficar, olhando, para, as, anotações, para, as...   \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  [maior, melhor, técnica, estudos, falam, técni...   \n",
       "1  [fez, assim, assim, aí, fez, quê, príncipe, aí...   \n",
       "2  [ficar, olhando, anotações, mãos, deprimente, ...   \n",
       "3  [4, coisas, precisa, saber, namora, pessoa, an...   \n",
       "\n",
       "                                            stemming  \\\n",
       "0  [é, a, mai, é, a, melhor, técn, e, os, estud, ...   \n",
       "1  [o, que, é, que, fez, assim, e, assim, e, voc,...   \n",
       "2  [fic, olh, par, as, anot, par, as, mão, é, dep...   \n",
       "3  [4, cois, que, voc, precis, sab, se, voc, nam,...   \n",
       "\n",
       "                                         lemmatizing  \n",
       "0  [ser, o, grande, ser, o, bom, técnica, e, o, e...  \n",
       "1  [o, que, ser, que, fazer, assim, e, assim, e, ...  \n",
       "2  [ficar, olhar, para, o, anotação, para, o, mão...  \n",
       "3  [4, coisa, que, você, precisar, saber, se, voc...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Baixar modelo de lingua portuguesa\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Carregar modelo de lingua portuguesa\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "df['lemmatizing'] = df['tokenized_words'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
