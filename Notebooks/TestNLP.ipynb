{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 42.0/42.0 kB 675.2 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp312-cp312-win_amd64.whl.metadata (165 kB)\n",
      "     ---------------------------------------- 0.0/165.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 165.5/165.5 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriel\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gabriel\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gabriel\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabriel\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-2.0.0-cp312-cp312-win_amd64.whl (16.2 MB)\n",
      "Downloading matplotlib-3.9.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.4/8.0 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.2/8.0 MB 33.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.8/8.0 MB 26.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/8.0 MB 39.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 36.3 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.4 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.5/2.6 MB 79.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 79.5 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.5/268.5 kB 17.2 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB ? eta 0:00:00\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, click, nltk, contourpy, matplotlib\n",
      "Successfully installed click-8.1.7 contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 joblib-1.4.2 kiwisolver-1.4.5 matplotlib-3.9.0 nltk-3.8.1 numpy-2.0.0 pillow-10.4.0 pyparsing-3.1.2 regex-2024.5.15 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gabriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['é que vocês vão ao banheiro e quando sai sai todo o calção todo enrolado a bermuda toda enrolada Ei eu acho que eu vou contratar uma pessoa sabe Para quê Quem é essa pessoa eu não sei para me ajudar a ensinar vocês a vestir a roupa porque eu já passei 43 anos e não consegui os calções é toda vez que vocês vão no banheiro deixa eu passar toda bagunça por que hein que vocês não se organizem hein então nem aí né que coisa é isso é palhaçada sua não é palhaçada precisa estar ajeitando as vezes a gente tá no shopping rapaz vocês vão ao banheiro quando sai sai todo bagunçado é quem gosta Ai gosta de fazer isso mas não tem vergonha não bicho eu não tenho nada e você não tenho vergonha tem mamãe para ajeitar né Você quer que eu suporte de tudo né']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "df = pd.read_csv(\"../CSV/TEA/TEA_corrigido/Transcricaoaudios.csv\", sep=';')\n",
    "df.head()\n",
    "sent_tokenize(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>Link</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é que vocês vão ao banheiro e quando sai sai t...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6fO5FfLmED/?ig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>encher as garrafas eu não posso Pois é meu fil...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C62Nj8BxD3W/?ig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hoje é o dia mundial da conscientização do aut...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C5RReZ9rzRm/?ig...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>droga</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6bnmfVuhHX/?ig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>consultar esse balãozinho de Deus ele vai fica...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6ICcZuoB68/?ig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription  tag  \\\n",
       "0   0  é que vocês vão ao banheiro e quando sai sai t...  TEA   \n",
       "1   1  encher as garrafas eu não posso Pois é meu fil...  TEA   \n",
       "2   2  hoje é o dia mundial da conscientização do aut...  TEA   \n",
       "3   3                                              droga  TEA   \n",
       "5   5  consultar esse balãozinho de Deus ele vai fica...  TEA   \n",
       "\n",
       "                                                Link classifier  \n",
       "0  https://www.instagram.com/reel/C6fO5FfLmED/?ig...          1  \n",
       "1  https://www.instagram.com/reel/C62Nj8BxD3W/?ig...          1  \n",
       "2  https://www.instagram.com/reel/C5RReZ9rzRm/?ig...          2  \n",
       "3  https://www.instagram.com/reel/C6bnmfVuhHX/?ig...          1  \n",
       "5  https://www.instagram.com/reel/C6ICcZuoB68/?ig...          1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using one hot encoding\n",
    "dictionary = dict({\n",
    "    \"experiência pessoal\": 1,\n",
    "    \"útil\": 2,\n",
    "    \"enganoso\": 3,\n",
    "    \"excluído (espanhol)\": 4\n",
    "})\n",
    "\n",
    "# Replacing in Classifier column and removing blank spaces in the end\n",
    "df['classifier'] = df['classifier'].str.rstrip()\n",
    "# Change to lower case\n",
    "df['classifier'] = df['classifier'].str.lower()\n",
    "# Converting to one hot encoding\n",
    "df['classifier'] = df['classifier'].replace(dictionary)\n",
    "df = df[~df['classifier'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "#Remove rows with 4 value\n",
    "df = df[df['classifier'] != 4]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>Link</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é que vocês vão ao banheiro e quando sai sai t...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6fO5FfLmED/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[é, que, vocês, vão, ao, banheiro, e, quando, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>encher as garrafas eu não posso Pois é meu fil...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C62Nj8BxD3W/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[encher, as, garrafas, eu, não, posso, Pois, é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hoje é o dia mundial da conscientização do aut...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C5RReZ9rzRm/?ig...</td>\n",
       "      <td>2</td>\n",
       "      <td>[hoje, é, o, dia, mundial, da, conscientização...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>droga</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6bnmfVuhHX/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[droga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>consultar esse balãozinho de Deus ele vai fica...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6ICcZuoB68/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[consultar, esse, balãozinho, de, Deus, ele, v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription  tag  \\\n",
       "0   0  é que vocês vão ao banheiro e quando sai sai t...  TEA   \n",
       "1   1  encher as garrafas eu não posso Pois é meu fil...  TEA   \n",
       "2   2  hoje é o dia mundial da conscientização do aut...  TEA   \n",
       "3   3                                              droga  TEA   \n",
       "5   5  consultar esse balãozinho de Deus ele vai fica...  TEA   \n",
       "\n",
       "                                                Link classifier  \\\n",
       "0  https://www.instagram.com/reel/C6fO5FfLmED/?ig...          1   \n",
       "1  https://www.instagram.com/reel/C62Nj8BxD3W/?ig...          1   \n",
       "2  https://www.instagram.com/reel/C5RReZ9rzRm/?ig...          2   \n",
       "3  https://www.instagram.com/reel/C6bnmfVuhHX/?ig...          1   \n",
       "5  https://www.instagram.com/reel/C6ICcZuoB68/?ig...          1   \n",
       "\n",
       "                                     tokenized_words  \n",
       "0  [é, que, vocês, vão, ao, banheiro, e, quando, ...  \n",
       "1  [encher, as, garrafas, eu, não, posso, Pois, é...  \n",
       "2  [hoje, é, o, dia, mundial, da, conscientização...  \n",
       "3                                            [droga]  \n",
       "5  [consultar, esse, balãozinho, de, Deus, ele, v...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_words'] = df['Transcription'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gabriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>Link</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é que vocês vão ao banheiro e quando sai sai t...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6fO5FfLmED/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[é, que, vocês, vão, ao, banheiro, e, quando, ...</td>\n",
       "      <td>[vão, banheiro, sai, sai, todo, calção, todo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>encher as garrafas eu não posso Pois é meu fil...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C62Nj8BxD3W/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[encher, as, garrafas, eu, não, posso, Pois, é...</td>\n",
       "      <td>[encher, garrafas, posso, Pois, filho, três, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hoje é o dia mundial da conscientização do aut...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C5RReZ9rzRm/?ig...</td>\n",
       "      <td>2</td>\n",
       "      <td>[hoje, é, o, dia, mundial, da, conscientização...</td>\n",
       "      <td>[hoje, dia, mundial, conscientização, autismo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>droga</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6bnmfVuhHX/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[droga]</td>\n",
       "      <td>[droga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>consultar esse balãozinho de Deus ele vai fica...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6ICcZuoB68/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[consultar, esse, balãozinho, de, Deus, ele, v...</td>\n",
       "      <td>[consultar, balãozinho, Deus, vai, ficar, bem,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription  tag  \\\n",
       "0   0  é que vocês vão ao banheiro e quando sai sai t...  TEA   \n",
       "1   1  encher as garrafas eu não posso Pois é meu fil...  TEA   \n",
       "2   2  hoje é o dia mundial da conscientização do aut...  TEA   \n",
       "3   3                                              droga  TEA   \n",
       "5   5  consultar esse balãozinho de Deus ele vai fica...  TEA   \n",
       "\n",
       "                                                Link classifier  \\\n",
       "0  https://www.instagram.com/reel/C6fO5FfLmED/?ig...          1   \n",
       "1  https://www.instagram.com/reel/C62Nj8BxD3W/?ig...          1   \n",
       "2  https://www.instagram.com/reel/C5RReZ9rzRm/?ig...          2   \n",
       "3  https://www.instagram.com/reel/C6bnmfVuhHX/?ig...          1   \n",
       "5  https://www.instagram.com/reel/C6ICcZuoB68/?ig...          1   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, que, vocês, vão, ao, banheiro, e, quando, ...   \n",
       "1  [encher, as, garrafas, eu, não, posso, Pois, é...   \n",
       "2  [hoje, é, o, dia, mundial, da, conscientização...   \n",
       "3                                            [droga]   \n",
       "5  [consultar, esse, balãozinho, de, Deus, ele, v...   \n",
       "\n",
       "                                   removed_stopwords  \n",
       "0  [vão, banheiro, sai, sai, todo, calção, todo, ...  \n",
       "1  [encher, garrafas, posso, Pois, filho, três, p...  \n",
       "2  [hoje, dia, mundial, conscientização, autismo,...  \n",
       "3                                            [droga]  \n",
       "5  [consultar, balãozinho, Deus, vai, ficar, bem,...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['removed_stopwords'] = df['tokenized_words'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.2380952380952381\n",
      "Recall:  0.3333333333333333\n",
      "F1-Score:  0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "df['classifier'] = df['classifier'].astype('int')\n",
    "y = df['classifier']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred, average='macro'))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>Link</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>tokenized_words_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é que vocês vão ao banheiro e quando sai sai t...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6fO5FfLmED/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[é, que, vocês, vão, ao, banheiro, e, quando, ...</td>\n",
       "      <td>[vão, banheiro, sai, sai, todo, calção, todo, ...</td>\n",
       "      <td>vão banheiro sai sai todo calção todo enrolado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>encher as garrafas eu não posso Pois é meu fil...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C62Nj8BxD3W/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[encher, as, garrafas, eu, não, posso, Pois, é...</td>\n",
       "      <td>[encher, garrafas, posso, Pois, filho, três, p...</td>\n",
       "      <td>encher garrafas posso Pois filho três porque s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hoje é o dia mundial da conscientização do aut...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C5RReZ9rzRm/?ig...</td>\n",
       "      <td>2</td>\n",
       "      <td>[hoje, é, o, dia, mundial, da, conscientização...</td>\n",
       "      <td>[hoje, dia, mundial, conscientização, autismo,...</td>\n",
       "      <td>hoje dia mundial conscientização autismo então...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>droga</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6bnmfVuhHX/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[droga]</td>\n",
       "      <td>[droga]</td>\n",
       "      <td>droga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>consultar esse balãozinho de Deus ele vai fica...</td>\n",
       "      <td>TEA</td>\n",
       "      <td>https://www.instagram.com/reel/C6ICcZuoB68/?ig...</td>\n",
       "      <td>1</td>\n",
       "      <td>[consultar, esse, balãozinho, de, Deus, ele, v...</td>\n",
       "      <td>[consultar, balãozinho, Deus, vai, ficar, bem,...</td>\n",
       "      <td>consultar balãozinho Deus vai ficar bem causa ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription  tag  \\\n",
       "0   0  é que vocês vão ao banheiro e quando sai sai t...  TEA   \n",
       "1   1  encher as garrafas eu não posso Pois é meu fil...  TEA   \n",
       "2   2  hoje é o dia mundial da conscientização do aut...  TEA   \n",
       "3   3                                              droga  TEA   \n",
       "5   5  consultar esse balãozinho de Deus ele vai fica...  TEA   \n",
       "\n",
       "                                                Link  classifier  \\\n",
       "0  https://www.instagram.com/reel/C6fO5FfLmED/?ig...           1   \n",
       "1  https://www.instagram.com/reel/C62Nj8BxD3W/?ig...           1   \n",
       "2  https://www.instagram.com/reel/C5RReZ9rzRm/?ig...           2   \n",
       "3  https://www.instagram.com/reel/C6bnmfVuhHX/?ig...           1   \n",
       "5  https://www.instagram.com/reel/C6ICcZuoB68/?ig...           1   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, que, vocês, vão, ao, banheiro, e, quando, ...   \n",
       "1  [encher, as, garrafas, eu, não, posso, Pois, é...   \n",
       "2  [hoje, é, o, dia, mundial, da, conscientização...   \n",
       "3                                            [droga]   \n",
       "5  [consultar, esse, balãozinho, de, Deus, ele, v...   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  [vão, banheiro, sai, sai, todo, calção, todo, ...   \n",
       "1  [encher, garrafas, posso, Pois, filho, três, p...   \n",
       "2  [hoje, dia, mundial, conscientização, autismo,...   \n",
       "3                                            [droga]   \n",
       "5  [consultar, balãozinho, Deus, vai, ficar, bem,...   \n",
       "\n",
       "                              tokenized_words_joined  \n",
       "0  vão banheiro sai sai todo calção todo enrolado...  \n",
       "1  encher garrafas posso Pois filho três porque s...  \n",
       "2  hoje dia mundial conscientização autismo então...  \n",
       "3                                              droga  \n",
       "5  consultar balãozinho Deus vai ficar bem causa ...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.2380952380952381\n",
      "Recall:  0.3333333333333333\n",
      "F1-Score:  0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "df['classifier'] = df['classifier'].astype('int')\n",
    "y = df['classifier']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000) \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred, average='macro'))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred, average='macro'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LogisticClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.2380952380952381\n",
      "Recall:  0.3333333333333333\n",
      "F1-Score:  0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "df['classifier'] = df['classifier'].astype('int')\n",
    "y = df['classifier']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RidgeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred, average='macro'))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.2380952380952381\n",
      "Recall:  0.3333333333333333\n",
      "F1-Score:  0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "df['classifier'] = df['classifier'].astype('int')\n",
    "y = df['classifier']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred, average='macro'))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.2380952380952381\n",
      "Recall:  0.3333333333333333\n",
      "F1-Score:  0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "df['tokenized_words_joined'] = df['removed_stopwords'].apply(lambda x: ' '.join(x))\n",
    "X = vectorizer.fit_transform(df['tokenized_words_joined'])\n",
    "df['classifier'] = df['classifier'].astype('int')\n",
    "y = df['classifier']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision: \", precision_score(y_val, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_val, y_pred, average='macro'))\n",
    "print(\"F1-Score: \", f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Deep learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deserialize' from 'keras.src.dtype_policies' (e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\keras\\src\\dtype_policies\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# creating a lstm training model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM, Embedding\n",
      "File \u001b[1;32me:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n",
      "File \u001b[1;32me:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\keras\\api\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtype_policies\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n",
      "File \u001b[1;32me:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\keras\\api\\dtype_policies\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype_policies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype_policies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype_policies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'deserialize' from 'keras.src.dtype_policies' (e:\\Gabriel\\UFAL\\Pesquisa\\MindVid_Research\\.venv\\Lib\\site-packages\\keras\\src\\dtype_policies\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.25))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.25))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Gabriel_Germano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "      <td>[maior, melhor, técnica, estudos, falam, técni...</td>\n",
       "      <td>[é, a, mai, é, a, melhor, técn, e, os, estud, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "      <td>[fez, assim, assim, aí, fez, quê, príncipe, aí...</td>\n",
       "      <td>[o, que, é, que, fez, assim, e, assim, e, voc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "      <td>[ficar, olhando, anotações, mãos, deprimente, ...</td>\n",
       "      <td>[fic, olh, par, as, anot, par, as, mão, é, dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "      <td>[4, coisas, precisa, saber, namora, pessoa, an...</td>\n",
       "      <td>[4, cois, que, voc, precis, sab, se, voc, nam,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...   \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...   \n",
       "2  [ficar, olhando, para, as, anotações, para, as...   \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  [maior, melhor, técnica, estudos, falam, técni...   \n",
       "1  [fez, assim, assim, aí, fez, quê, príncipe, aí...   \n",
       "2  [ficar, olhando, anotações, mãos, deprimente, ...   \n",
       "3  [4, coisas, precisa, saber, namora, pessoa, an...   \n",
       "\n",
       "                                            stemming  \n",
       "0  [é, a, mai, é, a, melhor, técn, e, os, estud, ...  \n",
       "1  [o, que, é, que, fez, assim, e, assim, e, voc,...  \n",
       "2  [fic, olh, par, as, anot, par, as, mão, é, dep...  \n",
       "3  [4, cois, que, voc, precis, sab, se, voc, nam,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "stemmer = RSLPStemmer()\n",
    "df['stemming'] = df['tokenized_words'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 107.3/107.3 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting jinja2 (from spacy)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabriel_germano\\documents\\pesquisa\\mindvid_research\\venv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.1.0-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Downloading spacy-3.7.4-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.7 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.7 MB 7.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.7 MB 8.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/11.7 MB 9.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/11.7 MB 9.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.8/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.2/11.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.0/11.7 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.7 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.7 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.7 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.7 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/11.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.9/11.7 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.7 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.7 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.7 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.7 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/11.7 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.0/182.0 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.4/122.4 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.3/409.3 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "   ---------------------------------------- 0.0/478.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 478.8/478.8 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.7/1.4 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 16.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.2/6.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.6/6.6 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.4 MB 17.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.4 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.8/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.1.0-cp312-cp312-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 150.9/150.9 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, marisa-trie, jinja2, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 jinja2-3.1.3 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>tag</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmatizing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>é a maior é a melhor técnica e os estudos fala...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[é, a, maior, é, a, melhor, técnica, e, os, es...</td>\n",
       "      <td>[maior, melhor, técnica, estudos, falam, técni...</td>\n",
       "      <td>[é, a, mai, é, a, melhor, técn, e, os, estud, ...</td>\n",
       "      <td>[ser, o, grande, ser, o, bom, técnica, e, o, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O que é que fez assim e assim e você aí você V...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[O, que, é, que, fez, assim, e, assim, e, você...</td>\n",
       "      <td>[fez, assim, assim, aí, fez, quê, príncipe, aí...</td>\n",
       "      <td>[o, que, é, que, fez, assim, e, assim, e, voc,...</td>\n",
       "      <td>[o, que, ser, que, fazer, assim, e, assim, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ficar olhando para as anotações para as mãos é...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[ficar, olhando, para, as, anotações, para, as...</td>\n",
       "      <td>[ficar, olhando, anotações, mãos, deprimente, ...</td>\n",
       "      <td>[fic, olh, par, as, anot, par, as, mão, é, dep...</td>\n",
       "      <td>[ficar, olhar, para, o, anotação, para, o, mão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 coisas que você precisa saber se você namora...</td>\n",
       "      <td>ansiedade</td>\n",
       "      <td>[4, coisas, que, você, precisa, saber, se, voc...</td>\n",
       "      <td>[4, coisas, precisa, saber, namora, pessoa, an...</td>\n",
       "      <td>[4, cois, que, voc, precis, sab, se, voc, nam,...</td>\n",
       "      <td>[4, coisa, que, você, precisar, saber, se, voc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Transcription        tag  \\\n",
       "0   0  é a maior é a melhor técnica e os estudos fala...  ansiedade   \n",
       "1   1  O que é que fez assim e assim e você aí você V...  ansiedade   \n",
       "2   2  ficar olhando para as anotações para as mãos é...  ansiedade   \n",
       "3   3  4 coisas que você precisa saber se você namora...  ansiedade   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [é, a, maior, é, a, melhor, técnica, e, os, es...   \n",
       "1  [O, que, é, que, fez, assim, e, assim, e, você...   \n",
       "2  [ficar, olhando, para, as, anotações, para, as...   \n",
       "3  [4, coisas, que, você, precisa, saber, se, voc...   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  [maior, melhor, técnica, estudos, falam, técni...   \n",
       "1  [fez, assim, assim, aí, fez, quê, príncipe, aí...   \n",
       "2  [ficar, olhando, anotações, mãos, deprimente, ...   \n",
       "3  [4, coisas, precisa, saber, namora, pessoa, an...   \n",
       "\n",
       "                                            stemming  \\\n",
       "0  [é, a, mai, é, a, melhor, técn, e, os, estud, ...   \n",
       "1  [o, que, é, que, fez, assim, e, assim, e, voc,...   \n",
       "2  [fic, olh, par, as, anot, par, as, mão, é, dep...   \n",
       "3  [4, cois, que, voc, precis, sab, se, voc, nam,...   \n",
       "\n",
       "                                         lemmatizing  \n",
       "0  [ser, o, grande, ser, o, bom, técnica, e, o, e...  \n",
       "1  [o, que, ser, que, fazer, assim, e, assim, e, ...  \n",
       "2  [ficar, olhar, para, o, anotação, para, o, mão...  \n",
       "3  [4, coisa, que, você, precisar, saber, se, voc...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Baixar modelo de lingua portuguesa\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Carregar modelo de lingua portuguesa\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "df['lemmatizing'] = df['tokenized_words'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
